# The type of game to be played [nim, hex]
game_type: hex

### Nim variables ###

# The number of pieces on the board
nim_N: 10
# The maximum number that a player can take off the board on their turn.
nim_K: 5

### Nim variables end ###

### Hex variables ###

# Recommended size in the interval [3, 10]
board_size: 4

### Hex variables end ###

episodes: 1

# How many simulations (i.e., search games) to perform per each actual move, where
# one simulation is a tree search followed by a rollout.
simulations: 10

### Parameters for the actor neural network (ANET) ###

learning_rate: 0.01

# List of neurons in each hidden layer. Set to an empty list if no hidden
# layers should be used. The input/output dimensions are automatically
# calculated based on board_size.
neurons_per_hidden_layer: [15, 15]

# Activation function for each layer (except the input layer)
# Options [linear, sigmoid, tanh, relu]
# For example a network with 2 hidden layers: [sigmoid, relu, relu]
activation_functions: [relu, relu, linear]

# [Adagrad, SGD, RMSProp, Adam]
# SGD = Stochastic Gradient Descent
optimizer: Adagrad

# How many random cases from replay buffer to train on, per episode
replay_buffer_selection_size: 100

# The batch size the neural network should train on. The random selection
# from the replay buffer is divided into these mini-batches.
# Should be <= replay_buffer_selection_size
mini_batch_size: 25

epochs: 10

# The number of ANETs to be cached in preparation for a Tournament of Progressive Policies (TOPP).
games_to_save: 5

# The number of games to be played between any two ANET-based agents that
# meet during the round-robin play of the TOPP.
games_between_agents: 5
