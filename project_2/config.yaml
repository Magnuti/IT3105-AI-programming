######################################################
### General ###
######################################################

# The type of game to be played [nim, hex]
game_type: hex

episodes: 150

# How many simulations (i.e., search games) to perform per each actual move, where
# one simulation is a tree search followed by a rollout.
simulations: 500

epsilon: 1

# How epsilon should decay [exponential, reversed_sigmoid, linear]
epsilon_decay_function: exponential
# The rate in which the epsilon decreases as given by e <- e * epsilon_decay
# Only used for epsilon_decay_function: exponential
epsilon_decay: 0.9955

visualize: True

######################################################
### Nim variables ###
######################################################

# The number of pieces on the board
nim_N: 10
# The maximum number that a player can take off the board on their turn.
nim_K: 5

######################################################
### Hex variables ###
######################################################

# Recommended size in the interval [3, 10]
board_size: 4

######################################################
### Parameters for the actor neural network (ANET) ###
######################################################

learning_rate: 0.01

# List of neurons in each hidden layer. Set to an empty list if no hidden
# layers should be used. The input/output dimensions are automatically
# calculated based on board_size.
neurons_per_hidden_layer: [15, 15]

# Activation function for each layer (except the input layer)
# Options [linear, sigmoid, tanh, relu, softmax]
# For example a network with 2 hidden layers: [sigmoid, relu, softmax]
# SoftMax is highly recommended in the output layer
activation_functions: [relu, relu, softmax]

# [Adagrad, SGD, RMSProp, Adam]
# SGD = Stochastic Gradient Descent
optimizer: SGD

# How many random cases from replay buffer to train on, per episode
replay_buffer_selection_size: 16

# The batch size the neural network should train on. The random selection
# from the replay buffer is divided into these mini-batches.
# Should be <= replay_buffer_selection_size
mini_batch_size: 16

epochs: 10

# The number of ANETs to be cached in preparation for a Tournament of Progressive Policies (TOPP).
games_to_save: 5

######################################################
### TOURNAMENTS ###
######################################################
# The number of games to be played between any two ANET-based agents that
# meet during the round-robin play of the TOPP.
games_between_agents: 5
