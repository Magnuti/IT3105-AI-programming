######################################################
### General ###
######################################################

# The type of game to be played [nim, hex]
game_type: hex

# True or False. if False, go straight to TOPP with presaved models
train: True

episodes: 25

# How many simulations (i.e., search games) to perform per each actual move, where
# one simulation is a tree search followed by a rollout.
simulations: 500
sim_timelimit: 6

# Explore constant for UTC, not used when dynamic explore bonus (c) is True
explore_constant: 1
# True | False
dynamic_explore_bonus: False

# 0 = random, 1 = weighted choice based on output_dist
rollout_explore: 0
epsilon: 1

# How epsilon should decay [exponential, reversed_sigmoid, linear]
epsilon_decay_function: linear
# The rate in which the epsilon decreases as given by e <- e * epsilon_decay
# Only used for epsilon_decay_function: exponential
epsilon_decay: 0.9955

visualize: False

######################################################
### Nim variables ###
######################################################

# The number of pieces on the board
nim_N: 10
# The maximum number that a player can take off the board on their turn.
nim_K: 5

######################################################
### Hex variables ###
######################################################

# Recommended size in the interval [3, 10]
board_size: 6

######################################################
### Parameters for the actor neural network (ANET) ###
######################################################

learning_rate: 0.01

# List of neurons in each hidden layer. Set to an empty list if no hidden
# layers should be used. The input/output dimensions are automatically
# calculated based on board_size.
neurons_per_hidden_layer: [64, 32]

# Activation function for each layer (except the input layer)
# Options [linear, sigmoid, tanh, relu, softmax]
# For example a network with 2 hidden layers: [sigmoid, relu, softmax]
# SoftMax is highly recommended in the output layer
activation_functions: [relu, relu, softmax]

# [Adagrad, SGD, RMSProp, Adam]
# SGD = Stochastic Gradient Descent
optimizer: Adam

replay_buffer_size: 254
# How many random cases from replay buffer to train on, per episode
replay_buffer_selection_size: 128

# The batch size the neural network should train on. The random selection
# from the replay buffer is divided into these mini-batches.
# Should be <= replay_buffer_selection_size
mini_batch_size: 16

epochs: 64

# The number of ANETs to be cached in preparation for a Tournament of Progressive Policies (TOPP).
games_to_save: 20

######################################################
### TOURNAMENTS ###
######################################################
# The number of games to be played between any two ANET-based agents that
# meet during the round-robin play of the TOPP.
games_between_agents: 5
